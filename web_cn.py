import os
import sys
import copy
import logging
import speechbrain as sb
import numpy as np
from hyperpyyaml import load_hyperpyyaml
import librosa
import soundfile as sf

os.environ["TOKENIZERS_PARALLELISM"] = "false"


logger = logging.getLogger(__name__)

from CustomGenerator import ASR


import time
import tempfile
import gradio as gr

# === æ–°å¢ï¼šè§£å†³ /tmp æƒé™é—®é¢˜ ===
custom_tmp = "/home/luxiangyu/DuplexMamba/tmp_gradio"
os.makedirs(custom_tmp, exist_ok=True)
tempfile.tempdir = custom_tmp
os.environ["GRADIO_TEMP_DIR"] = custom_tmp
print(f"ğŸ“ Gradio ä¸´æ—¶ç›®å½•: {custom_tmp}")
# =============================

# =====================
# ç¡®è®¤ä¸€ä¸‹è¯­éŸ³åˆ‡ç‰‡æ˜¯å¦æ­£ç¡®
# =====================
DEBUG_AUDIO_DIR = "/home/luxiangyu/DuplexMamba/debug_chunks"  # è‡ªå®šä¹‰è·¯å¾„
os.makedirs(DEBUG_AUDIO_DIR, exist_ok=True)

# å¯é€‰ï¼šå¦‚æœä½ è¦é‡é‡‡æ ·ï¼Œè¯·å®‰è£… scipy: pip install scipy
try:
    from scipy.signal import resample
    HAS_SCIPY = True
except ImportError:
    HAS_SCIPY = False
    print("âš ï¸ scipy æœªå®‰è£…ï¼Œæ— æ³•è‡ªåŠ¨é‡é‡‡æ ·ã€‚è¯·ç¡®ä¿éº¦å…‹é£è¾“å…¥ä¸º 16000Hzï¼Œæˆ–è¿è¡Œ: pip install scipy")

# =====================
# ğŸ”¹ é…ç½®å‚æ•°
# =====================
TARGET_SAMPLE_RATE = 16000      # æ¨¡å‹æœŸæœ›çš„é‡‡æ ·ç‡
CHUNK_DURATION_SEC = 3.0        # æ¯ä¸ªåˆ‡ç‰‡æ—¶é•¿ï¼ˆç§’ï¼‰
CHUNK_SAMPLES = int(TARGET_SAMPLE_RATE * CHUNK_DURATION_SEC)  # 32000



N_ahead = 4
IDLE = "idle"
PREFILL = "prefilling"
GENERATION = "generating"

model = None
model_copy = None

chat_history = ""
chat_history_copy = ""
pre_wav = None
cur_wav = None
last_token_embed = None
last_token_embed_copy = None
temp_model_kwargs = None # The hidden state cache for token prediction during the prefilling stage

prefix_prompt_feats = None
endofspeech_feats = None
assistant_prompt_feats = None
    

def duplex_voice_assistant(wav, chunk_id):
    # import pdb
    # pdb.set_trace()
    print(f"(chunk {chunk_id}) [è¯†åˆ«ç»“æœ] {os.path.basename(wav)}")
    web_result = ""  # output on web
    global model, model_copy
    global chat_history, chat_history_copy, pre_wav, cur_wav, last_token_embed, last_token_embed_copy, temp_model_kwargs
    global prefix_prompt_feats, endofspeech_feats, assistant_prompt_feats
    if wav is not None:
        audio_feats = model.audio_encoder(wav=wav)
    pre_wav = cur_wav
    cur_wav = wav
    print("USER: ",wav)


    # Determine the main and auxiliary model states based on the previous and current inputs
    # States remain unchanged for inputs 00 and 11
    if pre_wav is not None and cur_wav is None:
        # State 01 indicates that the previous audio has been fully input, 
        # forcing the main model into generating and the auxiliary model into idle 
        model.modules.Mamba.status = GENERATION
        model_copy.modules.Mamba.status = IDLE
        temp_model_kwargs = None  
    elif pre_wav is None and cur_wav is not None:
        # Switch between main and auxiliary models in 10 states, regardless of whether the main model has finished generating. 
        # This allows for seamless state rollback when ignoring speech.
        model_copy.modules.Mamba.status = PREFILL
        model_copy.modules.Mamba.model_kwargs = copy.deepcopy(model.modules.Mamba.model_kwargs)
        # The main and auxiliary models switch roles. 
        # The main model enters the prefilling state, while the auxiliary model remains unchanged, continuing to generate output or staying idle.
        model, model_copy = model_copy, model
        chat_history_copy = chat_history
        last_token_embed_copy = last_token_embed
        last_token_embed = None  
    
    # The main model state is either prefilling or generating. 
    # After generating, it automatically switches to idle without requiring explicit action.
    if model.modules.Mamba.status == PREFILL:  
        if model.modules.Mamba.model_kwargs == None:
            # Perform a single forward pass with parallel scanning to initialize the hidden state
            # In the prefill state, max_new_tokens controls the number of autoregressive loops. 
            # During the first prefill, model_kwargs is None, and max_new_tokens is set to 1; otherwise, it is set to 2.
            _ = model.modules.Mamba.duplex_generate(inputs_embeds=prefix_prompt_feats, max_new_tokens=1)
            chat_history += "<|user|>\nPlease answer the questions in the user's input speech.\n<|beginofspeech|>"
        elif temp_model_kwargs is None: # Indicates that the main model was in the generating state at the previous timestep
            if chat_history.endswith(model.tokenizer.eos_token):
                model.prefill(words="\n")
                chat_history += "\n"
            else:
                model.prefill(words="<|endoftext|>\n")
                chat_history += "<|endoftext|>\n"
            # Then, prefill the prefix_prompt section
            _ = model.modules.Mamba.duplex_generate(inputs_embeds=prefix_prompt_feats, max_new_tokens=2)
            chat_history += "<|user|>\nPlease answer the questions in the user's input speech.\n<|beginofspeech|>"

        # Perform token prediction; if the current audio input is complete, switch to the generating state.
        chat_history += "{" + cur_wav + "}"

        # Temporarily store the current audio slice and previous inputs as the prefill hidden state for seamless feature concatenation in the next loop.
        _ = model.modules.Mamba.duplex_generate(inputs_embeds=audio_feats, max_new_tokens=2)
        temp_model_kwargs = model.modules.Mamba.model_kwargs

        _ = model.modules.Mamba.duplex_generate(inputs_embeds=endofspeech_feats[:, :-1, :], max_new_tokens=2)
        # Temporarily set the model state to "generating" to predict a special token and determine if the audio input is complete.
        model.modules.Mamba.status = GENERATION
        # In generate mode, max_new_tokens defines the maximum length of the generated content.
        generated_token = model.modules.Mamba.duplex_generate(inputs_embeds=endofspeech_feats[:, -1:, :], do_sample=True, temperature=0.95, top_p=0.75, max_new_tokens=1)
        # Switching the model state back to prefilling
        model.modules.Mamba.status = PREFILL
        # The model's hidden state is restored to the prefilled state from the current audio slice and previous inputs
        model.modules.Mamba.model_kwargs = temp_model_kwargs

        print("*******************")
        print(model.tokenizer.decode(generated_token[0]))

        if model.tokenizer.decode(generated_token[0]) == "<|endofuser|>":
            # When the speech input is complete, the model switches to the generating state
            model.modules.Mamba.status = GENERATION
            model_copy.modules.Mamba.status = IDLE  
            temp_model_kwargs = None  
        elif model.tokenizer.decode(generated_token[0]) == "<|ignore|>":
            # The main model determines that the current input should be ignored
            # If the auxiliary model is in the generating state, simply set the main model to idle and switch roles to make the main model ignore the input speech.
            if model_copy.modules.Mamba.status == GENERATION:
                model.modules.Mamba.status = IDLE
                model.modules.Mamba.model_kwargs = copy.deepcopy(model_copy.modules.Mamba.model_kwargs)
                model, model_copy = model_copy, model
                chat_history = chat_history_copy
                last_token_embed = last_token_embed_copy
            else: # The auxiliary model is idle in its initial state, indicating that the first input should be ignored
                # The main model and historical data are reset to their initial states
                model.modules.Mamba.model_kwargs = None
                chat_history = ""
            temp_model_kwargs = None  


    # The auxiliary model can be either idle or generating. In the idle state, it performs no operations.
    if model_copy.modules.Mamba.status == GENERATION:
        if chat_history_copy != "" and not chat_history_copy.endswith(model.tokenizer.eos_token):
            # Generate only four tokens at a time
            out_copy = model_copy.modules.Mamba.duplex_generate(inputs_embeds=last_token_embed_copy, do_sample=True, temperature=0.8, top_p=0.7, max_new_tokens=N_ahead)
            last_token_id_copy = out_copy[:, -1].unsqueeze(0)
            last_token_embed_copy = model_copy.embedding_layer(last_token_id_copy)
            response_copy = model_copy.tokenizer.decode(out_copy[0][-N_ahead:])
            print("                                 Assistant_copy: " + response_copy)

            web_result = response_copy   # output on web
            
            chat_history_copy += response_copy


    if model.modules.Mamba.status == GENERATION:  # When the main model is in the generating state, the auxiliary model remains idle
        # Main model generation
        # Before generation, prefill with "<|endofspeech|><|endofuser|>\n<|assistant|>\n"
        # After embedding the last token, autoregressive generation begins
        if chat_history != "":
            if last_token_embed is None:
                model.modules.Mamba.status = PREFILL
                model.prefill(words="<|endofspeech|><|endofuser|>")
                chat_history += "<|endofspeech|><|endofuser|>"
                _ = model.modules.Mamba.duplex_generate(inputs_embeds=assistant_prompt_feats[:, :-1, :], max_new_tokens=2)
                chat_history += "\n<|assistant|>\n"
                model.modules.Mamba.status = GENERATION
                last_token_embed = assistant_prompt_feats[:, -1:, :]
            if not chat_history.endswith(model.tokenizer.eos_token):
                # Generate only four tokens at a time
                out = model.modules.Mamba.duplex_generate(inputs_embeds=last_token_embed, do_sample=True, temperature=0.8, top_p=0.7, max_new_tokens=N_ahead)
                last_token_id = out[:, -1].unsqueeze(0)
                last_token_embed = model.embedding_layer(last_token_id)
                response = model.tokenizer.decode(out[0][-N_ahead:])
                print("                                 Assistant: " + response)

                if web_result == "":   # output on web
                    web_result = response   
                else:
                    web_result = web_result + "\n" + response
                if web_result.endswith("<|endoftext|>"):
                    web_result = web_result[:-len("<|endoftext|>")] + "\n"
                
                chat_history += response
        
    print(chat_history)
    return web_result


# =====================
# ğŸ”¹ å·¥å…·å‡½æ•°
# =====================
def numpy_to_flac(numpy_audio, path, sr=TARGET_SAMPLE_RATE):
    pcm16 = (numpy_audio * 32767).astype(np.int16)
    sf.write(path, pcm16, sr, format="FLAC")
    return path

def get_latest_incremental_audio(temp_dir):
    """
    ä» Gradio ä¸´æ—¶ç›®å½•ä¸­æ‰¾å‡ºæœ€æ–°çš„ audio.wav æ–‡ä»¶ï¼ˆæŒ‰æ–‡ä»¶å¤¹ä¿®æ”¹æ—¶é—´ï¼‰
    """
    try:
        subdirs = [
            os.path.join(temp_dir, d)
            for d in os.listdir(temp_dir)
            if os.path.isdir(os.path.join(temp_dir, d))
        ]
        if not subdirs:
            return None
        # æŒ‰æ–‡ä»¶å¤¹ä¿®æ”¹æ—¶é—´æ’åºï¼Œå–æœ€æ–°
        latest_dir = max(subdirs, key=os.path.getmtime)
        audio_path = os.path.join(latest_dir, "audio.wav")
        if os.path.exists(audio_path):
            return audio_path
        else:
            return None
    except Exception as e:
        print(f"âš ï¸ [ERROR] è·å–æœ€æ–°éŸ³é¢‘å¤±è´¥: {e}")
        return None

# =====================
# ğŸ”¹ æ¸…ç©ºå†å²å‡½æ•°
# =====================
def clear_history(state):
    """æ¸…ç©ºç½‘é¡µæ˜¾ç¤ºçš„è¯†åˆ«ç»“æœ"""
    if state is None:
        state = {"history_text": ""}
    else:
        state["history_text"] = ""
    return "", state

# =====================
# ğŸ”¹ æ ¸å¿ƒå¤„ç†å‡½æ•°ï¼ˆå¸¦è¯¦ç»†æ—¥å¿—ï¼‰
# =====================
def transcribe_stream(audio, state):
    """
    æ³¨æ„ï¼šè™½ç„¶æ¥æ”¶ audio å‚æ•°ï¼ˆGradio å¼ºåˆ¶ä¼ å…¥ï¼‰ï¼Œä½†æˆ‘ä»¬å¿½ç•¥å®ƒï¼Œ
    è½¬è€Œä» GRADIO_TEMP_DIR ä¸­è¯»å–çœŸå®çš„å¢é‡ audio.wav æ–‡ä»¶ã€‚
    """
    print("\nğŸ™ï¸ [DEBUG] transcribe_stream è¢«è°ƒç”¨")

    # ä» Gradio ä¸´æ—¶ç›®å½•è¯»å–æœ€æ–°å¢é‡ç‰‡æ®µ
    latest_audio_path = get_latest_incremental_audio(custom_tmp)
    if latest_audio_path is None:
        print("âš ï¸ [DEBUG] æœªæ‰¾åˆ°æœ€æ–°çš„ audio.wavï¼Œè·³è¿‡å¤„ç†")
        # è¿”å›å½“å‰å†å²ï¼ˆé¿å…æ¸…ç©ºï¼‰
        current_text = state.get("history_text", "") if state else ""
        return current_text, state

    print(f"ğŸ”Š [DEBUG] è¯»å–å¢é‡éŸ³é¢‘: {latest_audio_path}")
    data, sr = sf.read(latest_audio_path, dtype='float32')
    print(f"ğŸ“Š [DEBUG] å¢é‡éŸ³é¢‘ - é‡‡æ ·ç‡: {sr} Hz, é•¿åº¦: {len(data)} samples ({len(data)/sr:.2f}s)")

    # å¤„ç†ç«‹ä½“å£° â†’ å•å£°é“
    if len(data.shape) > 1:
        print("ğŸ”Š [DEBUG] æ£€æµ‹åˆ°å¤šå£°é“ï¼Œå–å¹³å‡è½¬å•å£°é“")
        data = data.mean(axis=1)

    # é‡é‡‡æ ·åˆ° TARGET_SAMPLE_RATEï¼ˆå¦‚æœéœ€è¦ï¼‰
    if sr != TARGET_SAMPLE_RATE:
        if HAS_SCIPY:
            print(f"ğŸ”„ [DEBUG] é‡é‡‡æ ·: {sr}Hz â†’ {TARGET_SAMPLE_RATE}Hz")
            num_samples = int(len(data) * TARGET_SAMPLE_RATE / sr)
            data = resample(data, num_samples)
            sr = TARGET_SAMPLE_RATE
        else:
            error_msg = "âŒ [ERROR] é‡‡æ ·ç‡ä¸åŒ¹é…ä¸”æ— æ³•é‡é‡‡æ ·ï¼ˆè¯·å®‰è£… scipyï¼‰"
            current_text = state.get("history_text", "") if state else ""
            return current_text + "\n" + error_msg, state

    # === åˆå§‹åŒ–æˆ–æ›´æ–° state ===
    current_time = time.time()
    if state is None:
        print("ğŸ†• [DEBUG] åˆå§‹åŒ–æ–°ä¼šè¯çŠ¶æ€")
        state = {
            "last_time": current_time,
            "buffer": np.zeros(0, dtype=np.float32),
            "chunk_counter": 0,
            "history_text": ""
        }
    else:
        time_gap = current_time - state["last_time"]
        # è§„åˆ™ï¼šè¶…è¿‡10ç§’æ²¡æ•°æ® â†’ æ–°ä¼šè¯
        if time_gap > 10.0:
            print("ğŸ†• [DEBUG] è¶…æ—¶ï¼ˆ>10sï¼‰ï¼Œé‡ç½®ä¸ºæ–°ä¼šè¯ï¼Œä¿ç•™å†å²æ–‡æœ¬")
            state = {
                "last_time": current_time,
                "buffer": np.zeros(0, dtype=np.float32),
                "chunk_counter": 0,
                "history_text": state.get("history_text", "")
            }
        else:
            state["last_time"] = current_time

    # === æ–°å¢éŸ³é¢‘å°±æ˜¯æ•´ä¸ª dataï¼ˆå› ä¸ºæ¥è‡ªå¢é‡ç‰‡æ®µï¼‰===
    new_data = data
    print(f"ğŸ†• [DEBUG] æ–°å¢éŸ³é¢‘é•¿åº¦: {len(new_data)} samples")

    # ç´¯ç§¯åˆ° buffer
    buffer = np.concatenate([state["buffer"], new_data])
    print(f"ğŸ§º [DEBUG] buffer æ€»é•¿åº¦: {len(buffer)} samples (ç›®æ ‡: â‰¥{CHUNK_SAMPLES})")

    # === åˆ‡ç‰‡å¤„ç† ===
    chunk_counter = state["chunk_counter"]
    results = ""
    while len(buffer) >= CHUNK_SAMPLES:
        chunk = buffer[:CHUNK_SAMPLES]
        buffer = buffer[CHUNK_SAMPLES:]
        chunk_counter += 1
        print(f"âœ‚ï¸ [DEBUG] åˆ‡ç‰‡ #{chunk_counter}ï¼Œé€å…¥æ¨¡å‹")

        # === ä¿å­˜è°ƒè¯•ç”¨éŸ³é¢‘æ–‡ä»¶ ===
        debug_path = os.path.join(DEBUG_AUDIO_DIR, f"chunk_{chunk_counter:03d}.flac")
        numpy_to_flac(chunk, debug_path, sr=TARGET_SAMPLE_RATE)
        print(f"ğŸ’¾ [DEBUG] å·²ä¿å­˜è°ƒè¯•éŸ³é¢‘: {debug_path}")

        with tempfile.NamedTemporaryFile(delete=False, suffix=".flac") as f:
            flac_path = numpy_to_flac(chunk, f.name, sr=TARGET_SAMPLE_RATE)
            text = duplex_voice_assistant(flac_path, chunk_counter)
            print(f"ğŸ¤– [DEBUG] æ¨¡å‹è¿”å›: {text}")
            results += text
            try:
                os.unlink(flac_path)
            except Exception as e:
                print(f"ğŸ—‘ï¸ [WARN] ä¸´æ—¶æ–‡ä»¶åˆ é™¤å¤±è´¥: {e}")

    # æ›´æ–°çŠ¶æ€
    state["buffer"] = buffer
    state["chunk_counter"] = chunk_counter

    # è¿½åŠ æ–°ç»“æœåˆ°å†å²
    if results.strip():
        state["history_text"] += results

    print(f"ğŸ“¤ [DEBUG] å½“å‰å®Œæ•´å†å²é•¿åº¦: {len(state['history_text'])} å­—ç¬¦")
    return state["history_text"], state

# =====================
# ğŸ”¹ Gradio UI
# =====================
with gr.Blocks(title="å®æ—¶è¯­éŸ³åŠ©æ‰‹") as demo:
    gr.Markdown("## ğŸ™ï¸ å®æ—¶è¯­éŸ³è¯†åˆ«åŠ©æ‰‹ï¼ˆæ¯3ç§’åˆ‡ç‰‡ï¼‰")
    gr.Markdown("ç‚¹å‡»éº¦å…‹é£å¼€å§‹è¯´è¯ï¼Œç³»ç»Ÿå°†æ¯2ç§’å¤„ç†ä¸€æ¬¡éŸ³é¢‘ã€‚")

    audio_input = gr.Audio(
        source="microphone",
        type="numpy",          # ä¿ç•™ type="numpy"ï¼ˆGradio è¦æ±‚ streaming å¿…é¡»ä¼  audioï¼‰
        streaming=True,
        label="ğŸ™ï¸ å®æ—¶è¯­éŸ³è¾“å…¥"
    )
    output_text = gr.Textbox(label="ğŸ—£ï¸ å®æ—¶è¯†åˆ«ç»“æœ", lines=10, interactive=False)
    state = gr.State(None)

    # æµå¼è¯†åˆ«
    audio_input.stream(
        fn=transcribe_stream,
        inputs=[audio_input, state],
        outputs=[output_text, state]
    )
    # æ¸…ç©ºæŒ‰é’®
    clear_btn = gr.Button("ğŸ—‘ï¸ æ¸…ç©ºæ–‡æœ¬")
    clear_btn.click(
        fn=clear_history,
        inputs=[state],
        outputs=[output_text, state]
    )



if __name__ == "__main__":
    # CLI:
    hparams_file, run_opts, overrides = sb.parse_arguments(sys.argv[1:])
    with open(hparams_file) as fin:
        hparams = load_hyperpyyaml(fin, overrides)
    # create ddp_group with the right communication protocol
    sb.utils.distributed.ddp_init_group(run_opts)
        
    hparams["modules"]["CNN"].requires_grad_(False)
    hparams["modules"]["Transformer"].requires_grad_(False)
    hparams["modules"]["Speech_Adapter"].requires_grad_(False)
    hparams_copy = copy.deepcopy(hparams)

    model = ASR(modules=hparams["modules"], hparams=hparams, run_opts=run_opts)                 # The main model
    model_copy = ASR(modules=hparams_copy["modules"], hparams=hparams_copy, run_opts=run_opts)  # The auxiliary model

    model.modules.Mamba.status = PREFILL    # The main model
    model_copy.modules.Mamba.status = IDLE  # The auxiliary model

    prefix_prompt_feats = model.embedding_layer(model.prefix_prompt)
    endofspeech_feats = model.embedding_layer(model.endofspeech)
    assistant_prompt_feats = model.embedding_layer(model.assistant_prompt)
    
    demo.queue().launch(
        server_name="0.0.0.0",  # å…è®¸å¤–ç½‘è®¿é—®
        server_port=7860         # è‡ªå®šä¹‰ç«¯å£
    )